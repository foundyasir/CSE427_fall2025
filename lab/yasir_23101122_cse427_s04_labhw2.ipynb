{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8VpwU4SP6dh"
      },
      "source": [
        "Implementing Decision Tree and Random Forest from Scratch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "JfMpK4YIP-cD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4ef1178-d10c-4345-dd30-2bbb4a353e49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier (from scratch) Accuracy: 0.8192090395480226\n"
          ]
        }
      ],
      "source": [
        "#1\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/titanic.csv')\n",
        "\n",
        "# Dropped redundant/unnecessary columns\n",
        "df = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
        "\n",
        "# Handled missing values\n",
        "df['Age'] = df['Age'].fillna(df['Age'].mean())  # Filled Age with mean value\n",
        "df.dropna(subset=['Embarked'], inplace=True)      # Dropped the rows with missing value of Embarked\n",
        "\n",
        "# Hot encoding for categorical columns\n",
        "df = pd.get_dummies(df, columns=['Sex', 'Embarked'])\n",
        "X = df.drop(['Survived'], axis=1).values.astype(float)\n",
        "y = df['Survived'].values.astype(int)\n",
        "\n",
        "def training_testing(X, y, testsize=0.2, randomstate=42):\n",
        "    random_number_generate = np.random.RandomState(randomstate)\n",
        "    index = np.arange(len(y))\n",
        "    random_number_generate.shuffle(index)\n",
        "    n_test = int(len(y) * testsize)\n",
        "    testing_index = index[:n_test]\n",
        "    training_index = index[n_test:]\n",
        "    return X[training_index], X[testing_index], y[training_index], y[testing_index]\n",
        "X_train_np, X_test_np, y_train_np, y_test_np = training_testing(X, y, testsize=0.2, randomstate=42)\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = 2\n",
        "        self.tree_ = None           # will be holding the learned tree dictonary\n",
        "        self.max_features = None\n",
        "\n",
        "    def gini_impurity(self, y):\n",
        "        if len(y) == 0:\n",
        "            return 0.0\n",
        "        category, counts = np.unique(y, return_counts=True)\n",
        "        probability = counts / counts.sum()\n",
        "        return 1.0 - np.sum(probability ** 2)\n",
        "\n",
        "    def information_gain(self, y_parent, y_left, y_right, parent=None):\n",
        "        if parent is None:\n",
        "            parent = self.gini_impurity(y_parent)\n",
        "        n = len(y_parent)\n",
        "        n_l, n_r = len(y_left), len(y_right)\n",
        "        if n_l == 0 or n_r == 0:\n",
        "            return 0.0\n",
        "        weighted = (n_l / n) * self.gini_impurity(y_left) + (n_r / n) * self.gini_impurity(y_right)\n",
        "        return parent - weighted\n",
        "\n",
        "\n",
        "    def threshold_candidate_determine(self, values):\n",
        "        uniq_val = np.unique(values)\n",
        "        if len(uniq_val) <= 1:\n",
        "            return []\n",
        "        uniq_val.sort()\n",
        "        return (uniq_val[:-1] + uniq_val[1:]) / 2.0\n",
        "\n",
        "    def best_spliting_finding(self, X, y, feature_index=None):\n",
        "        n_samples, n_features = X.shape\n",
        "        if feature_index is None:\n",
        "            feature_index = np.arange(n_features)\n",
        "        best_gain = 0.0\n",
        "        best_feature = None\n",
        "        best_threshold = None\n",
        "        parent = self.gini_impurity(y)\n",
        "\n",
        "        for feature in feature_index:\n",
        "            col = X[:, feature]\n",
        "            threshold = self.threshold_candidate_determine(col)\n",
        "            for thres in threshold:\n",
        "                left = col <= thres\n",
        "                right = ~left\n",
        "                if left.sum() == 0 or right.sum() == 0:\n",
        "                    continue\n",
        "                gain = self.information_gain(y, y[left], y[right], parent)\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feature = feature\n",
        "                    best_threshold = thres\n",
        "        return best_feature, best_threshold\n",
        "\n",
        "    def majority_class(self, y):\n",
        "        values, counts = np.unique(y, return_counts=True)\n",
        "        return int(values[np.argmax(counts)])\n",
        "\n",
        "    def fit(self, X, y, depth=0):\n",
        "        if len(np.unique(y)) == 1:\n",
        "            leaf = {'type': 'leaf', 'class': int(y[0])}\n",
        "            if depth == 0:\n",
        "                self.tree_ = leaf\n",
        "            return leaf\n",
        "\n",
        "        if (self.max_depth is not None and depth >= self.max_depth) or (len(y) < self.min_samples_split):\n",
        "            leaf = {'type': 'leaf', 'class': self.majority_class(y)}\n",
        "            if depth == 0:\n",
        "                self.tree_ = leaf\n",
        "            return leaf\n",
        "\n",
        "        n_features = X.shape[1]\n",
        "        if self.max_features is None or self.max_features >= n_features:\n",
        "            feature_index = None  # using all features\n",
        "        else:\n",
        "            feature_index = np.random.choice(n_features, self.max_features, replace=False)\n",
        "\n",
        "        # Finding best split\n",
        "        best_feature, best_threshold = self.best_spliting_finding(X, y, feature_index)\n",
        "\n",
        "        if best_feature is None:\n",
        "            leaf = {'type': 'leaf', 'class': self.majority_class(y)}\n",
        "            if depth == 0:\n",
        "                self.tree_ = leaf\n",
        "            return leaf\n",
        "\n",
        "        # Partitioning the data\n",
        "        left = X[:, best_feature] <= best_threshold\n",
        "        right = ~left\n",
        "\n",
        "        # Recursively building children\n",
        "        left_child = self.fit(X[left], y[left], depth + 1)\n",
        "        right_child = self.fit(X[right], y[right], depth + 1)\n",
        "\n",
        "        node = {\n",
        "            'type': 'node',\n",
        "            'feat': int(best_feature),\n",
        "            'threshold': float(best_threshold),\n",
        "            'left': left_child,\n",
        "            'right': right_child\n",
        "        }\n",
        "        if depth == 0:\n",
        "            self.tree_ = node\n",
        "        return node\n",
        "\n",
        "    def prediction(self, tree, instance):\n",
        "        node = tree\n",
        "        while node['type'] != 'leaf':\n",
        "            feature = node['feat']\n",
        "            thres = node['threshold']\n",
        "            if instance[feature] <= thres:\n",
        "                node = node['left']\n",
        "            else:\n",
        "                node = node['right']\n",
        "        return node['class']\n",
        "\n",
        "    def prediction_whole(self, X):\n",
        "        return np.array([self.prediction(self.tree_, row) for row in X])\n",
        "\n",
        "\n",
        "# ------------------ Train/Evaluate DecisionTree ------------------\n",
        "decision_tree = DecisionTree(max_depth=5)\n",
        "final_tree = decision_tree.fit(X_train_np, y_train_np)  # returns the tree structure\n",
        "\n",
        "# predictionions on the test set\n",
        "y_pred_dt_scr = np.array([decision_tree.prediction(final_tree, row) for row in X_test_np])\n",
        "\n",
        "# Calculate accuracy manually: correctly predictioned / total\n",
        "correct = np.sum(y_pred_dt_scr == y_test_np)\n",
        "accuracy_dt_scr = correct / len(y_test_np)\n",
        "\n",
        "print(\"Decision Tree Classifier (from scratch) Accuracy:\", accuracy_dt_scr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "m_4CHmbEQAkL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12cea3f8-e9e9-42a7-b30c-3b56a96b9677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier (from scratch) Accuracy: 0.8305084745762712\n"
          ]
        }
      ],
      "source": [
        "#2\n",
        "import numpy as np\n",
        "\n",
        "class RandomForest:\n",
        "    def __init__(self, n_estimate=100, max_depth=None):\n",
        "        self.n_estimate = n_estimate\n",
        "        self.max_depth = max_depth\n",
        "        self.trees_ = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.trees_.clear()\n",
        "        n_features = X.shape[1]\n",
        "        # Used sqrt(d) features at each split\n",
        "        max_feature = int(np.sqrt(n_features))\n",
        "        if max_feature < 1:\n",
        "            max_feature = 1\n",
        "\n",
        "        for _ in range(self.n_estimate):\n",
        "            X_sample, y_sample = self.bootstrap_sampling(X, y)\n",
        "            tree = DecisionTree(max_depth=self.max_depth)\n",
        "            tree.max_feature = max_feature\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees_.append(tree)\n",
        "\n",
        "    def bootstrap_sampling(self, X, y):\n",
        "        n = len(y)\n",
        "        idx = np.random.randint(0, n, size=n)\n",
        "        return X[idx], y[idx]\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Collected predictions from each tree; shape (n_estimate, n_samples)\n",
        "        predictions = np.vstack([tree.prediction_whole(X) for tree in self.trees_])\n",
        "        predicted = []\n",
        "        for i in range(predictions.shape[1]):\n",
        "            values, counts = np.unique(predictions[:, i], return_counts=True)\n",
        "            predicted.append(values[np.argmax(counts)])\n",
        "        return np.array(predicted)\n",
        "\n",
        "\n",
        "# ------------------ Train/Evaluate RandomForest ------------------\n",
        "random_forest = RandomForest(n_estimate=25, max_depth=5)\n",
        "random_forest.fit(X_train_np, y_train_np)\n",
        "\n",
        "# Predictions on the test set\n",
        "y_pred_rf_scr = random_forest.predict(X_test_np)\n",
        "\n",
        "# Calculate accuracy manually\n",
        "correct_rf = np.sum(y_pred_rf_scr == y_test_np)\n",
        "accuracy_rf_scr = correct_rf / len(y_test_np)\n",
        "\n",
        "print(\"Random Forest Classifier (from scratch) Accuracy:\", accuracy_rf_scr)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}